## 01 BP神经网络的数据分类 语音特征信号分类

### 1 BP神经网络简介

#### 1.1 BP神经网络是什么

BP（Back-propagation，反向传播）神经网络是最传统的神经网络。也就是使用了Back-propagation算法的神经网络。请注意他不是时下流行的那一套深度学习。而其实机器学习的瓶颈就是成功的突破了非常深的神经网络无法用BP算法来训练的问题。

那么反向传播的东西是什么呢？答案是：误差。就是在模拟过程中（这是一个循环，我们在训练神经网络的时候是要不断的去重复这个过程的）收集系统所产生的**误差，**并且返回这些误差到输出值，之后用这些误差来调整神经元的权重，这样生成一个可以模拟出原始问题的人工神经网络系统。

#### 1.2 生物神经的运作原理

我在这里不讨论那些复杂的生物学和神经科学。其实很简单的一件事，我们人类之所以可以让飞机上天，是因为”学习“了从理论力学，信号系统再到导航控制的一系列知识。作家之所以可以写出伟大的著作，那是因为他学习了语言，比如英语汉语德语法语，也学习过李白杜甫，莎士比亚。其实狗狗之所以能知道要听见主人喊一个苹果加一个苹果要喊两声（之后可以得到一块炖牛肉），也是学习的结果。

那么学习的过程，核心问题是什么？答案是误差。比如你是一个备考高三数学的学生。你本身就会1+1=2，你把这个训练一万遍有用么？没用。你要做的就是找到自己不行的地方，找到自己的漏洞，有针对性的去突破和训练。这就是用误差来学习。

人类的过程也是一样的。学习，有误差然后有反馈，我们通过这些误差反馈来学习。

#### 1.3 神经网络的基础架构

神经网络其实就是几层神经元，每层神经元里有几个神经元点。不同layer之间的神经元相互连接。其实就是如此：

<img src="https://pic2.zhimg.com/80/v2-f9dd7a92df2d444ce93a711edda2d915_1440w.jpg" style="zoom:67%;" />

每一个神经元就是三件事：输入，判断和输出。输入层的神经元（就是那个圆形的圈，代表一个神经元或者一个神经细胞）是读入你输入的数据的。只要你有数据，这个玩意就能跑。这就好比你只要有汽油，汽车就能开是一个道理。中间则是”隐含层“，你可以控制这个隐含层的层数，以及每一层里有多少个神经元或者神经细胞。当然在实际操作里为了方便我们一般都直接认为你不管用几层，每层的神经元或者神经细胞数目都是一样的。因为这样的话写代码会比较方便。

每一层神经元内部都不互相连接。而相邻层的神经元点之间则互相连接。在我们这个问题里，两个相邻层，所有的神经元都是相互连接的。你说可不可以通过让这些神经元之间不互相连接来起到效果？的确，历史上的连接学派就是这样想的。但实际上你可以都给他联上。其实道理非常简单。如果我们真的要取消某两个点之间的连接的话，那么很显然只要设定这条连线上的数值为零即可。这好比一个网络电路，阻值本身就是无限大的。

除了神经元，你还需要关注一个东西，那便是神经线。在所有的神经线（两个神经元一连就是）上你可以赋予不同的权重。

而这个则是训练的核心要务，说白了就是得出最接近完美答案的权重。

#### 1.4 运算过程：矩阵乘法

我们要把输入在这个数学结构上传递到输出，要怎么办呢？答案是，使用矩阵乘法。

其实这样的思路一开始是非常简单的。 我们以这个例子来说明。

<img src="https://pic2.zhimg.com/80/v2-f9dd7a92df2d444ce93a711edda2d915_1440w.jpg" style="zoom:50%;" />

在这幅图里，我们把上一层的第i个神经元和下一层的第j个神经元之间的权重记为 w(ij)。而把上一层传入的三个input，分别记为S(1)，S(2)和S(3)。

因为数据肯定是在不同的层之间流动的。我们所做的就是通过矩阵乘法求解下一层的输出数值。

这个过程其实也非常简单。我们把输出记为O(1)，O(2)和O(3)。

结合权重，以O(1)举例。O(1)里的输出自然有来自S(1)，S(2)，S(3)的。那么分别按照权重去乘就可以了。权重自然就是一个大于等于零的实数嘛。
$$
O(1)=S(1)*w（11）+S(2)*w(21)+S(3)*w(31)
$$
同理，我们可以求解出O(2)和O(3)。
$$
O(2)=S(1)*w（12）+S(2)*w(22)+S(3)*w(32)
$$

$$
O(3)=S(1)*w（13）+S(2)*w(23)+S(3)*w(33)
$$

没错，这就是一个矩阵的乘法！我们使用矩阵乘法，可以一层一层的把最左边输入的数据送到最右边来。

我们自然可以得到一个结果。但是这个结果有可能是错的吧！所以怎么办呢？我们用真实的结果去和这个结果去比如求差，自然就可以把误差求解出来了。

同样的道理，我们可以把误差按照矩阵乘法一路返回。而且在这个过程中，每个神经元和神经线自然可以得到一些”信息“。我们可以用这些信息来修正神经网络，其实也就是给所有的边去赋予不同的权重。

如法炮制。在你有误差E（1），E（2），E（3）的时候，自然可以用这些误差返回上一层，得到上一层应该被返回的误差。其实也是一个矩阵乘法的内容。

### 2 数据选择和归一化

首先根据倒谱系数法提取四类音乐语音特征信号，不同的语音信号分别用1，2，3，4标识。

<img src="https://img2018.cnblogs.com/i-beta/1752446/202002/1752446-20200217164145385-1441774121.png" style="zoom:50%;" />

每组数据为25维，第1维维类别标识，后24维为语音特征信号。然后把四类语音信号何为一组，抽取1500组数据作为训练数据，500组数据作为测试数据，并对数据进行归一化处理。根据语音类别标识设定每组语音信号的期望输出值，如标识类为1时，期望输出向量为[1 0 0 0]。

```matlab
%下载四类语音信号
load data1 c1
load data2 c2
load data3 c3
load data4 c4

%四个特征信号矩阵合成一个矩阵
data(1:500,:)=c1(1:500,:);
data(501:1000,:)=c2(1:500,:);
data(1001:1500,:)=c3(1:500,:);
data(1501:2000,:)=c4(1:500,:);

%从1到2000间随机排序
k=rand(1,2000);
[m,n]=sort(k);

%输入输出数据
input=data(:,2:25);
output1 =data(:,1);

%把输出从1维变成4维
output=zeros(2000,4);
for i=1:2000
    switch output1(i)
        case 1
            output(i,:)=[1 0 0 0];
        case 2
            output(i,:)=[0 1 0 0];
        case 3
            output(i,:)=[0 0 1 0];
        case 4
            output(i,:)=[0 0 0 1];
    end
end

%随机提取1500个样本为训练样本，500个样本为预测样本
input_train=input(n(1:1500),:)';
output_train=output(n(1:1500),:)';
input_test=input(n(1501:2000),:)';
output_test=output(n(1501:2000),:)';

%输入数据归一化
[inputn,inputps]=mapminmax(input_train);
```

### 3 BP神经网络结构初始化

根据语音特征信号特点确定BP神经网络的结构为24-25-4，随机初始化BP神经网络权值和阈值。

```matlab
innum=24;
midnum=25;
outnum=4;

%权值初始化
w1=rands(midnum,innum);
b1=rands(midnum,1);
w2=rands(midnum,outnum);
b2=rands(outnum,1);

w2_1=w2;w2_2=w2_1;
w1_1=w1;w1_2=w1_1;
b1_1=b1;b1_2=b1_1;
b2_1=b2;b2_2=b2_1;

%学习率
xite=0.1;
alfa=0.01;
loopNumber=10;
I=zeros(1,midnum);
Iout=zeros(1,midnum);
FI=zeros(1,midnum);
dw1=zeros(innum,midnum);
db1=zeros(1,midnum);
```

### 4 训练

 用训练数据训练BP神经网络，在训练过程中根据网络预测误差调整网络的权值和阈值。

```matlab
E=zeros(1,loopNumber);
for ii=1:loopNumber
    E(ii)=0;
    for i=1:1:1500
       %% 网络预测输出 
        x=inputn(:,i);
        % 隐含层输出
        for j=1:1:midnum
            I(j)=inputn(:,i)'*w1(j,:)'+b1(j);
            Iout(j)=1/(1+exp(-I(j)));
        end
        % 输出层输出
        yn=w2'*Iout'+b2;
        
       %% 权值阀值修正
        %计算误差
        e=output_train(:,i)-yn;     
        E(ii)=E(ii)+sum(abs(e));
        
        %计算权值变化率
        dw2=e*Iout;
        db2=e';
        
        for j=1:1:midnum
            S=1/(1+exp(-I(j)));
            FI(j)=S*(1-S);
        end      
        for k=1:1:innum
            for j=1:1:midnum
                dw1(k,j)=FI(j)*x(k)*(e(1)*w2(j,1)+e(2)*w2(j,2)+e(3)*w2(j,3)+e(4)*w2(j,4));
                db1(j)=FI(j)*(e(1)*w2(j,1)+e(2)*w2(j,2)+e(3)*w2(j,3)+e(4)*w2(j,4));
            end
        end
           
        w1=w1_1+xite*dw1';
        b1=b1_1+xite*db1';
        w2=w2_1+xite*dw2';
        b2=b2_1+xite*db2';
        
        w1_2=w1_1;w1_1=w1;
        w2_2=w2_1;w2_1=w2;
        b1_2=b1_1;b1_1=b1;
        b2_2=b2_1;b2_1=b2;
    end
end
 

%% 语音特征信号分类
inputn_test=mapminmax('apply',input_test,inputps);
fore=zeros(4,500);
for ii=1:1
    for i=1:500%1500
        %隐含层输出
        for j=1:1:midnum
            I(j)=inputn_test(:,i)'*w1(j,:)'+b1(j);
            Iout(j)=1/(1+exp(-I(j)));
        end
        
        fore(:,i)=w2'*Iout'+b2;
    end
end
```

### 5 结果分析

```matlab
%根据网络输出找出数据属于哪类
output_fore=zeros(1,500);
for i=1:500
    output_fore(i)=find(fore(:,i)==max(fore(:,i)));
end

%BP网络预测误差
error=output_fore-output1(n(1501:2000))';
```

#### 5.1 预测语音种类和实际语音种类的分类图

```matlab
figure(1)
plot(output_fore,'r')
hold on
plot(output1(n(1501:2000))','b')
legend('预测语音类别','实际语音类别')
```

<img src="https://img2018.cnblogs.com/i-beta/1752446/202002/1752446-20200217161554805-71975384.png" style="zoom:50%;" />

#### 5.2 误差图

```matlab
figure(2)
plot(error)
title('BP网络分类误差','fontsize',12)
xlabel('语音信号','fontsize',12)
ylabel('分类误差','fontsize',12)
```

<img src="https://img2018.cnblogs.com/i-beta/1752446/202002/1752446-20200217161613901-1683931140.png" style="zoom:50%;" />

10次迭代过程中，误差绝对值之和的变化趋势：

<img src="https://img2018.cnblogs.com/i-beta/1752446/202002/1752446-20200217204618555-1238769116.png" style="zoom:50%;" />

